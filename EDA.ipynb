{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport glob\nimport json\n\nimport cv2\nimport torch\nfrom tqdm import tqdm_notebook\nfrom PIL import Image, ImageFile\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"META_DIR = \"/kaggle/input/iwildcam2022-fgvc9/metadata/metadata/\"\nTRAIN_DIR = \"/kaggle/input/iwildcam2022-fgvc9/train/train/\"\n\ntrain_data = json.load(open(META_DIR + 'iwildcam2022_train_annotations.json'))\ntest_data = json.load(open(META_DIR + 'iwildcam2022_test_information.json'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.DataFrame({'id': [item['id'] for item in train_data['annotations']],\n                                'category_id': [item['category_id'] for item in train_data['annotations']],\n                                'image_id': [item['image_id'] for item in train_data['annotations']],\n                                'seq_id': [item['seq_id'] for item in train_data['images']],\n                                'file_name': [item['file_name'] for item in train_data['images']],\n                                'location': [item['location'] for item in train_data['images']],\n                                'seq_num_frames': [item['seq_num_frames'] for item in train_data['images']],\n                                'seq_frame_num': [item['seq_frame_num'] for item in train_data['images']]})\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# import tqdm\n\n# indices = []\n# for i in tdqm(df_train['file_name']):\n#     try:\n#         Image.open(TRAIN_DIR + i)\n#     except:        \n#         print(i)\n#         df_train.drop(df_train.loc[df_train['file_name']==i].index, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_count = pd.read_csv(META_DIR + 'train_sequence_counts.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_count.head","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as T\nimport torch.nn.functional as F","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://github.com/ultralytics/yolov3/blob/master/utils/datasets.py code for yolo dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms = T.Compose(\n           [\n            T.ConvertImageDtype(torch.float32),\n            T.Normalize(mean=0.5, std=0.5),  # map [0, 1] into [-1, 1]\n            T.Resize(size=(1024, 1024)),\n           ])\n\nclass CamDataset(Dataset):\n    \n    def __init__(self,\n                 df,\n                 images_dir,\n                 seq_count,\n                 img_size=1024\n                ):\n        self.df = df\n        self.images_dir = images_dir\n        self.seq_count = seq_count\n        self.img_size = img_size\n    \n    def __len__(self):\n        return self.seq_count.shape[0]\n    \n    def __getitem__(self, idx): \n        cur_idx_row = self.seq_count.iloc[idx]\n        y = int(cur_idx_row['count'])\n        img_rows = self.df.loc[self.df.seq_id == cur_idx_row['seq_id']]\n        out = torch.zeros(1024,1024,3,img_rows.iloc[0].seq_num_frames)\n        for index, img_row in img_rows.iterrows():\n           img_path = os.path.join(self.images_dir, img_row['file_name'])\n           \n           out[:,:,:,img_row['seq_frame_num']] = transforms(torchvision.io.read_image(img_path)).permute(1,2,0)\n        padded_out = F.pad(out, pad=(10 - out.shape[3], 0, 0, 0, 0, 0, 0, 0))\n        return padded_out, y    \n\n        \n            \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def collate_fn(data):\n#     \"\"\"\n#        data: is a list of tuples with (example, label, length)\n#              where 'example' is a tensor of arbitrary shape\n#              and label/length are scalars\n#     \"\"\"\n#     examples, labels = zip(*data)\n#     F.pad(source, pad=(0, 0, 0, 70 - source.shape[0])\n\n#     return features.float(), labels.long(), lengths.long()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntraining_data = CamDataset(df_train, TRAIN_DIR, seq_count)\ntrain_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_features, train_labels = next(iter(train_dataloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install cython; pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -r '/kaggle/input/centertrack/requirements.txt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd /kaggle/input/centertrack/src/lib/model/networks/\n!git clone https://github.com/CharlesShang/DCNv2/ # clone if it is not automatically downloaded by `--recursive`.\n!cd DCNv2\n!./make.sh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%tb\nimport sys\nCENTERTRACK_PATH = '/kaggle/input/centertrack/src/lib/'\nsys.path.insert(0, CENTERTRACK_PATH)\n\nfrom detector import Detector\nfrom opts import opts\n\nMODEL_PATH = '/kaggle/input/centertrackmodel/coco_tracking.pth'\nTASK = 'tracking,ddd' # or 'tracking,multi_pose' for pose tracking and 'tracking,ddd' for monocular 3d tracking\nopt = opts().init('{} --load_model {}'.format(TASK, MODEL_PATH).split(' '))\ndetector = Detector(opt)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = ['''image read from open cv or from a video''']\nfor img in images:\n  ret = detector.run(img)['results']","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}