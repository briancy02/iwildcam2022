{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce347ef1-badf-495c-a13d-418c39010340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/briancy2/.conda/envs/iwildcam/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b0daca-a62f-495e-8de7-554fddf08f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261428/261428 [00:00<00:00, 1499703.90it/s]\n"
     ]
    }
   ],
   "source": [
    "META_DIR = \"data/metadata/metadata/\"\n",
    "TRAIN_DIR = \"data/train/train/\"\n",
    "TEST_DIR = \"data/test/test/\"\n",
    "\n",
    "test_data = json.load(open(META_DIR + 'iwildcam2022_test_information.json'))\n",
    "train_data = json.load(open(META_DIR + 'iwildcam2022_train_annotations.json'))\n",
    "\n",
    "#Test images\n",
    "df_test = pd.DataFrame({'id': [item['id'] for item in test_data['images']],\n",
    "                                'seq_id': [item['seq_id'] for item in test_data['images']],\n",
    "                                'file_name': [item['file_name'] for item in test_data['images']],\n",
    "                                'location': [item['location'] for item in test_data['images']],\n",
    "                                'seq_num_frames': [item['seq_num_frames'] for item in test_data['images']],\n",
    "                                'seq_frame_num': [item['seq_frame_num'] for item in test_data['images']]})\n",
    "#Train images\n",
    "df_train = pd.DataFrame({'id': [item['id'] for item in train_data['images']],\n",
    "                                'seq_id': [item['seq_id'] for item in train_data['images']],\n",
    "                                'file_name': [item['file_name'] for item in train_data['images']],\n",
    "                                'location': [item['location'] for item in train_data['images']],\n",
    "                                'seq_num_frames': [item['seq_num_frames'] for item in train_data['images']],\n",
    "                                'seq_frame_num': [item['seq_frame_num'] for item in train_data['images']]})\n",
    "# Detection for train test\n",
    "detections = json.load(open(META_DIR+\"iwildcam2022_mdv4_detections.json\"))['images']\n",
    "det_dict = dict()\n",
    "for detection in tqdm(detections):\n",
    "    det_dict[detection['file']] = detection['detections']\n",
    "df_detection = pd.DataFrame({'file': [item['file'] for item in detections],\n",
    "                                'detections': [item['detections'] for item in detections]})\n",
    "# Test sequence ids\n",
    "test_sequence_ids = pd.unique(df_test['seq_id'])\n",
    "# Train sequence id and count\n",
    "train_seq_count = pd.read_csv(META_DIR+\"train_sequence_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5a056d-82f7-45c3-a4b0-13c4b5eb8ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: img2vec_pytorch in /home/briancy2/.conda/envs/iwildcam/lib/python3.9/site-packages (1.0.1)\n",
      "Requirement already satisfied: torchvision in /home/briancy2/.conda/envs/iwildcam/lib/python3.9/site-packages (from img2vec_pytorch) (0.11.2)\n",
      "Requirement already satisfied: torch in /home/briancy2/.conda/envs/iwildcam/lib/python3.9/site-packages (from img2vec_pytorch) (1.10.1)\n",
      "Requirement already satisfied: numpy in /home/briancy2/.conda/envs/iwildcam/lib/python3.9/site-packages (from img2vec_pytorch) (1.20.3)\n",
      "Requirement already satisfied: typing_extensions in /home/briancy2/.conda/envs/iwildcam/lib/python3.9/site-packages (from torch->img2vec_pytorch) (3.7.4.3)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/briancy2/.conda/envs/iwildcam/lib/python3.9/site-packages (from torchvision->img2vec_pytorch) (8.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install img2vec_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa5bba-a681-4c3c-8459-5484f3f1c1fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 47/11028 [00:06<41:35,  4.40it/s]/home/briancy2/iwildcam2022/deep_sort/deep_sort/ocsort.py:41: RuntimeWarning: invalid value encountered in sqrt\n",
      "  w = np.sqrt(x[2] * x[3])\n",
      " 10%|█         | 1106/11028 [04:13<28:48,  5.74it/s] "
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from deep_sort.deep_sort.ocsort import OCSort\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "file = open('submission_above4tracker0.3.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "for sequence_id in tqdm(test_sequence_ids):\n",
    "    #tracker = Tracker(metric)\n",
    "    #results = []\n",
    "    img_rows = df_test.loc[df_test.seq_id == sequence_id]\n",
    "    tracker = OCSort(det_thresh=0.3, max_age=30, min_hits=3, \n",
    "        iou_threshold=0, delta_t=3, asso_func=\"iou\", inertia=0.2, use_byte=False)\n",
    "    trackers = []\n",
    "    d_count = []\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        image = Image.open(TEST_DIR+img_row['file_name'])\n",
    "        detections = df_detection.loc[df_detection.file == ('test/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        d_count.append(len([d for d in detections if d[\"conf\"] > 0.95]))\n",
    "        dets = np.zeros((1, 5))\n",
    "        detections = [d[\"bbox\"] + [d[\"conf\"]] for d in detections if d[\"conf\"] < 0.95]\n",
    "        if detections:\n",
    "            dets = np.array(detections)\n",
    "        output = tracker.update(dets, image.size, image.size)\n",
    "        #print([t.id for t in tracker.trackers])\n",
    "        trackers.extend(tracker.trackers)\n",
    "        \n",
    "        #print(d_count)\n",
    "    if max(d_count) < 4:   \n",
    "        count = max(d_count)\n",
    "    else: \n",
    "        count = len(set([t.id for t in trackers])) + max(d_count)\n",
    "    file.write('\\n' + str(sequence_id) + ',' + str(count))\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2615ccf6-59b3-42ba-860c-4d4a71f36250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from deep_sort.deep_sort.ocsort import OCSort\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "file = open('submission_above5tracker93.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "for sequence_id in tqdm(test_sequence_ids):\n",
    "    #tracker = Tracker(metric)\n",
    "    #results = []\n",
    "    img_rows = df_test.loc[df_test.seq_id == sequence_id]\n",
    "    tracker = OCSort(det_thresh=0.8, max_age=30, min_hits=3, \n",
    "        iou_threshold=0, delta_t=3, asso_func=\"iou\", inertia=0.2, use_byte=False)\n",
    "    trackers = []\n",
    "    d_count = []\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        image = Image.open(TEST_DIR+img_row['file_name'])\n",
    "        detections = df_detection.loc[df_detection.file == ('test/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        d_count.append(len([d for d in detections if d[\"conf\"] > 0.93]))\n",
    "        dets = np.zeros((1, 5))\n",
    "        detections = [d[\"bbox\"] + [d[\"conf\"]] for d in detections if d[\"conf\"] < 0.93]\n",
    "        if detections:\n",
    "            dets = np.array(detections)\n",
    "        output = tracker.update(dets, image.size, image.size)\n",
    "        #print([t.id for t in tracker.trackers])\n",
    "        trackers.extend(tracker.trackers)\n",
    "        \n",
    "        #print(d_count)\n",
    "    if max(d_count) < 5:   \n",
    "        count = max(d_count)\n",
    "    else: \n",
    "        count = len(set([t.id for t in trackers])) + max(d_count)\n",
    "    file.write('\\n' + str(sequence_id) + ',' + str(count))\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa542c44-bb4e-4a56-944e-a34993cdac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from deep_sort.deep_sort.ocsort import OCSort\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "file = open('submission_above4tracker93.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "for sequence_id in tqdm(test_sequence_ids):\n",
    "    #tracker = Tracker(metric)\n",
    "    #results = []\n",
    "    img_rows = df_test.loc[df_test.seq_id == sequence_id]\n",
    "    tracker = OCSort(det_thresh=0.3, max_age=30, min_hits=3, \n",
    "        iou_threshold=0, delta_t=3, asso_func=\"iou\", inertia=0.2, use_byte=False)\n",
    "    trackers = []\n",
    "    d_count = []\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        image = Image.open(TEST_DIR+img_row['file_name'])\n",
    "        detections = df_detection.loc[df_detection.file == ('test/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        d_count.append(len([d for d in detections if d[\"conf\"] > 0.95]))\n",
    "        dets = np.zeros((1, 5))\n",
    "        detections = [d[\"bbox\"] + [d[\"conf\"]] for d in detections if d[\"conf\"] < 0.95]\n",
    "        if detections:\n",
    "            dets = np.array(detections)\n",
    "        output = tracker.update(dets, image.size, image.size)\n",
    "        #print([t.id for t in tracker.trackers])\n",
    "        trackers.extend(tracker.trackers)\n",
    "        \n",
    "        #print(d_count)\n",
    "    if max(d_count) < 4:   \n",
    "        count = max(d_count)\n",
    "    else: \n",
    "        count = len(set([t.id for t in trackers])) + max(d_count)\n",
    "    file.write('\\n' + str(sequence_id) + ',' + str(count))\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b28f55-b778-48de-aaa5-f9555ecce123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from deep_sort.deep_sort.ocsort import OCSort\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "file = open('submission_above4tracker_det0.5.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "for sequence_id in tqdm(test_sequence_ids):\n",
    "    #tracker = Tracker(metric)\n",
    "    #results = []\n",
    "    img_rows = df_test.loc[df_test.seq_id == sequence_id]\n",
    "    tracker = OCSort(det_thresh=0.5, max_age=30, min_hits=3, \n",
    "        iou_threshold=0, delta_t=3, asso_func=\"iou\", inertia=0.2, use_byte=False)\n",
    "    trackers = []\n",
    "    d_count = []\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        image = Image.open(TEST_DIR+img_row['file_name'])\n",
    "        detections = df_detection.loc[df_detection.file == ('test/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        d_count.append(len([d for d in detections if d[\"conf\"] > 0.95]))\n",
    "        dets = np.zeros((1, 5))\n",
    "        detections = [d[\"bbox\"] + [d[\"conf\"]] for d in detections if d[\"conf\"] < 0.95]\n",
    "        if detections:\n",
    "            dets = np.array(detections)\n",
    "        output = tracker.update(dets, image.size, image.size)\n",
    "        #print([t.id for t in tracker.trackers])\n",
    "        trackers.extend(tracker.trackers)\n",
    "        \n",
    "        #print(d_count)\n",
    "    if max(d_count) < 4:   \n",
    "        count = max(d_count)\n",
    "    else: \n",
    "        count = len(set([t.id for t in trackers])) + max(d_count)\n",
    "    file.write('\\n' + str(sequence_id) + ',' + str(count))\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854a2e0-c55c-424c-ab3b-7479dead453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from deep_sort.deep_sort.ocsort import OCSort\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "file = open('submission_above5tracker_det0.75.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "for sequence_id in tqdm(test_sequence_ids):\n",
    "    #tracker = Tracker(metric)\n",
    "    #results = []\n",
    "    img_rows = df_test.loc[df_test.seq_id == sequence_id]\n",
    "    tracker = OCSort(det_thresh=0.75, max_age=30, min_hits=3, \n",
    "        iou_threshold=0, delta_t=3, asso_func=\"iou\", inertia=0.2, use_byte=False)\n",
    "    trackers = []\n",
    "    d_count = []\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        image = Image.open(TEST_DIR+img_row['file_name'])\n",
    "        detections = df_detection.loc[df_detection.file == ('test/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        d_count.append(len([d for d in detections if d[\"conf\"] > 0.95]))\n",
    "        dets = np.zeros((1, 5))\n",
    "        detections = [d[\"bbox\"] + [d[\"conf\"]] for d in detections if d[\"conf\"] < 0.95]\n",
    "        if detections:\n",
    "            dets = np.array(detections)\n",
    "        output = tracker.update(dets, image.size, image.size)\n",
    "        #print([t.id for t in tracker.trackers])\n",
    "        trackers.extend(tracker.trackers)\n",
    "        \n",
    "        #print(d_count)\n",
    "    if max(d_count) < 4:   \n",
    "        count = max(d_count)\n",
    "    else: \n",
    "        count = len(set([t.id for t in trackers])) + max(d_count)\n",
    "    file.write('\\n' + str(sequence_id) + ',' + str(count))\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3697ab13-b1a2-4635-aa5a-01b4250a5271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 5861]\n",
      " [   1 3888]\n",
      " [   2  761]\n",
      " [   3  237]\n",
      " [   4  109]\n",
      " [   5   59]\n",
      " [   6   43]\n",
      " [   7   21]\n",
      " [   8   14]\n",
      " [   9   11]\n",
      " [  10    5]\n",
      " [  11    6]\n",
      " [  12    4]\n",
      " [  13    5]\n",
      " [  14    1]\n",
      " [  15    2]\n",
      " [  16    1]]\n"
     ]
    }
   ],
   "source": [
    "c = pd.read_csv('submission_res.txt')[\"Predicted\"].to_numpy()\n",
    "print(np.asarray(np.unique(c, return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b55d6a-e874-46df-94e2-b2d5891c0fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 6111]\n",
      " [   1 4112]\n",
      " [   2  538]\n",
      " [   3  161]\n",
      " [   4   47]\n",
      " [   5   32]\n",
      " [   6   12]\n",
      " [   7    4]\n",
      " [   8    6]\n",
      " [   9    4]\n",
      " [  10    1]]\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv('submission.txt')[\"Predicted\"].to_numpy()\n",
    "print(np.asarray(np.unique(c, return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7acd6-7661-4270-a722-0c699a6e61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7af39c1-0587-4bc3-b3d1-945924d7724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1780it [16:07,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from deep_sort.application_util import preprocessing\n",
    "from deep_sort.application_util import visualization\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "from deep_sort.deep_sort.tracker import Tracker\n",
    "from img2vec_pytorch import Img2Vec\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "print(len(train_seq_count))\n",
    "file = open('submission_deep_conf_0.5.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "counts = []\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", 0.5, 100)\n",
    "for index, seq_row in tqdm(train_seq_count.iterrows()) :\n",
    "    tracker = Tracker(metric)\n",
    "    results = []\n",
    "    track_list = []\n",
    "    img_rows = df_train.loc[df_train.seq_id == seq_row[\"seq_id\"]]\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        detections = df_detection.loc[df_detection.file == ('train/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        image = Image.open(TRAIN_DIR+img_row['file_name'])\n",
    "        #print(detections[0][\"bbox\"])\n",
    "        #x = detections[0][\"bbox\"][1]+detections[0][\"bbox\"][0]\n",
    "        #detections[0][\"bbox\"][0]:x\n",
    "        #print(x)\n",
    "        #print(image[0:int(image.shape[1]*x),3:5,:])\n",
    "        #,detections[0][\"bbox\"][2]-detections[0][\"bbox\"][3]:detections[0][\"bbox\"][2],:\n",
    "        detections = [Detection(tuple(det[\"bbox\"]), det[\"conf\"], img2vec.get_vec(image.crop((int(image.size[0]*det[\"bbox\"][0]),int(image.size[0]*(det[\"bbox\"][1]+det[\"bbox\"][0])),int(image.size[1]*det[\"bbox\"][2]),int(image.size[1]*(det[\"bbox\"][2]-det[\"bbox\"][3])))), tensor=True)[0,:,0,0]) for det in detections if det[\"conf\"] > 0.5]\n",
    "#         boxes = np.array([d for d in filtered_det])\n",
    "#         scores = np.array([d.confidence for d in detections])\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            results.append(track.track_id)\n",
    "    count = len(set(results))   \n",
    "    counts.append(count)\n",
    "    file.write('\\n' + str(seq_row[\"seq_id\"]) + ',' + str(count))\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e1ec2-c77f-4df6-b424-22533832b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.application_util import preprocessing\n",
    "from deep_sort.application_util import visualization\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "from deep_sort.deep_sort.tracker import Tracker\n",
    "from img2vec_pytorch import Img2Vec\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "img2vec = Img2Vec(cuda=True)\n",
    "\n",
    "print(len(train_seq_count))\n",
    "file = open('submission_deep_conf_0.7.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "counts = []\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", 0.5, 100)\n",
    "for index, seq_row in tqdm(train_seq_count.iterrows()) :\n",
    "    tracker = Tracker(metric)\n",
    "    results = []\n",
    "    track_list = []\n",
    "    img_rows = df_train.loc[df_train.seq_id == seq_row[\"seq_id\"]]\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        detections = df_detection.loc[df_detection.file == ('train/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        image = Image.open(TRAIN_DIR+img_row['file_name'])\n",
    "        #print(detections[0][\"bbox\"])\n",
    "        #x = detections[0][\"bbox\"][1]+detections[0][\"bbox\"][0]\n",
    "        #detections[0][\"bbox\"][0]:x\n",
    "        #print(x)\n",
    "        #print(image[0:int(image.shape[1]*x),3:5,:])\n",
    "        #,detections[0][\"bbox\"][2]-detections[0][\"bbox\"][3]:detections[0][\"bbox\"][2],:\n",
    "        detections = [Detection(tuple(det[\"bbox\"]), det[\"conf\"], img2vec.get_vec(image.crop((int(image.size[0]*det[\"bbox\"][0]),int(image.size[0]*(det[\"bbox\"][1]+det[\"bbox\"][0])),int(image.size[1]*det[\"bbox\"][2]),int(image.size[1]*(det[\"bbox\"][2]-det[\"bbox\"][3])))), tensor=True)[0,:,0,0]) for det in detections if det[\"conf\"] > 0.7]\n",
    "#         boxes = np.array([d for d in filtered_det])\n",
    "#         scores = np.array([d.confidence for d in detections])\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            results.append(track.track_id)\n",
    "    count = len(set(results))   \n",
    "    counts.append(count)\n",
    "    file.write('\\n' + str(seq_row[\"seq_id\"]) + ',' + str(count))\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cddfbed8-adec-46f0-8823-f449941001ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:17,  1.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3226970/4131992462.py\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#print(image[0:int(image.shape[1]*x),3:5,:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#,detections[0][\"bbox\"][2]-detections[0][\"bbox\"][3]:detections[0][\"bbox\"][2],:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetections\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conf\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m#         boxes = np.array([d for d in filtered_det])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#         scores = np.array([d.confidence for d in detections])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3226970/4131992462.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#print(image[0:int(image.shape[1]*x),3:5,:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#,detections[0][\"bbox\"][2]-detections[0][\"bbox\"][3]:detections[0][\"bbox\"][2],:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetections\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conf\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m#         boxes = np.array([d for d in filtered_det])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#         scores = np.array([d.confidence for d in detections])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/iwildcam/lib/python3.9/site-packages/img2vec_pytorch/img_to_vec.py\u001b[0m in \u001b[0;36mget_vec\u001b[0;34m(self, img, tensor)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextraction_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mh_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/iwildcam/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/iwildcam/lib/python3.9/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/iwildcam/lib/python3.9/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/iwildcam/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/iwildcam/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/iwildcam/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/iwildcam/lib/python3.9/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/iwildcam/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/iwildcam/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/iwildcam/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from deep_sort.application_util import preprocessing\n",
    "from deep_sort.application_util import visualization\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "from deep_sort.deep_sort.tracker import Tracker\n",
    "from img2vec_pytorch import Img2Vec\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "img2vec = Img2Vec(cuda=True)\n",
    "\n",
    "print(len(train_seq_count))\n",
    "file = open('submission_deep_conf_0.3.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "counts = []\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", 0.5, 100)\n",
    "for index, seq_row in tqdm(train_seq_count.iterrows()) :\n",
    "    tracker = Tracker(metric)\n",
    "    results = []\n",
    "    track_list = []\n",
    "    img_rows = df_train.loc[df_train.seq_id == seq_row[\"seq_id\"]]\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        detections = df_detection.loc[df_detection.file == ('train/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        image = Image.open(TRAIN_DIR+img_row['file_name'])\n",
    "        #print(detections[0][\"bbox\"])\n",
    "        #x = detections[0][\"bbox\"][1]+detections[0][\"bbox\"][0]\n",
    "        #detections[0][\"bbox\"][0]:x\n",
    "        #print(x)\n",
    "        #print(image[0:int(image.shape[1]*x),3:5,:])\n",
    "        #,detections[0][\"bbox\"][2]-detections[0][\"bbox\"][3]:detections[0][\"bbox\"][2],:\n",
    "        detections = [Detection(tuple(det[\"bbox\"]), det[\"conf\"], img2vec.get_vec(image.crop((int(image.size[0]*det[\"bbox\"][0]),int(image.size[0]*(det[\"bbox\"][1]+det[\"bbox\"][0])),int(image.size[1]*det[\"bbox\"][2]),int(image.size[1]*(det[\"bbox\"][2]-det[\"bbox\"][3])))), tensor=True)[0,:,0,0]) for det in detections if det[\"conf\"] > 0.3]\n",
    "#         boxes = np.array([d for d in filtered_det])\n",
    "#         scores = np.array([d.confidence for d in detections])\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            results.append(track.track_id)\n",
    "    count = len(set(results))   \n",
    "    counts.append(count)\n",
    "    file.write('\\n' + str(seq_row[\"seq_id\"]) + ',' + str(count))\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8e3c2c0-b2f2-44fe-8b3f-e72d54e8f66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1780it [13:54,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from deep_sort.application_util import preprocessing\n",
    "from deep_sort.application_util import visualization\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "from deep_sort.deep_sort.tracker import Tracker\n",
    "from img2vec_pytorch import Img2Vec\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "img2vec = Img2Vec(cuda=True)\n",
    "\n",
    "print(len(train_seq_count))\n",
    "file = open('submission_deep_conf_0.5_nn0.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "counts = []\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", 0, 100)\n",
    "for index, seq_row in tqdm(train_seq_count.iterrows()) :\n",
    "    tracker = Tracker(metric)\n",
    "    results = []\n",
    "    track_list = []\n",
    "    img_rows = df_train.loc[df_train.seq_id == seq_row[\"seq_id\"]]\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        detections = df_detection.loc[df_detection.file == ('train/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        image = Image.open(TRAIN_DIR+img_row['file_name'])\n",
    "        #print(detections[0][\"bbox\"])\n",
    "        #x = detections[0][\"bbox\"][1]+detections[0][\"bbox\"][0]\n",
    "        #detections[0][\"bbox\"][0]:x\n",
    "        #print(x)\n",
    "        #print(image[0:int(image.shape[1]*x),3:5,:])\n",
    "        #,detections[0][\"bbox\"][2]-detections[0][\"bbox\"][3]:detections[0][\"bbox\"][2],:\n",
    "        detections = [Detection(tuple(det[\"bbox\"]), det[\"conf\"], img2vec.get_vec(image.crop((int(image.size[0]*det[\"bbox\"][0]),int(image.size[0]*(det[\"bbox\"][1]+det[\"bbox\"][0])),int(image.size[1]*det[\"bbox\"][2]),int(image.size[1]*(det[\"bbox\"][2]-det[\"bbox\"][3])))), tensor=True)[0,:,0,0]) for det in detections if det[\"conf\"] > 0.5]\n",
    "#         boxes = np.array([d for d in filtered_det])\n",
    "#         scores = np.array([d.confidence for d in detections])\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            results.append(track.track_id)\n",
    "    count = len(set(results))   \n",
    "    counts.append(count)\n",
    "    file.write('\\n' + str(seq_row[\"seq_id\"]) + ',' + str(count))\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2ecc246-6b46-4079-98a4-83d26e00c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.read_csv('submission_deep_conf_0.5_nn0.txt')[\"Predicted\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77c88ea5-8c3a-4bb5-81d8-c529c78507c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.mean(np.abs((np.array(counts) - train_seq_count[\"count\"].to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bfd77e8-9aa7-4683-962b-7e55cbe10be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3780898876404495\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04824d0c-a7a8-4467-905d-24eec63926eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -9   3]\n",
      " [ -8  20]\n",
      " [ -7  22]\n",
      " [ -6  33]\n",
      " [ -5  46]\n",
      " [ -4  61]\n",
      " [ -3  99]\n",
      " [ -2 179]\n",
      " [ -1 443]\n",
      " [  0 627]\n",
      " [  1 197]\n",
      " [  2  37]\n",
      " [  3  10]\n",
      " [  4   1]\n",
      " [  5   1]\n",
      " [  6   1]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique((np.array(counts) - train_seq_count[\"count\"]), return_counts=True)\n",
    "\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9554f3-ba20-4dd0-8af7-ca6fbe605d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.application_util import preprocessing\n",
    "from deep_sort.application_util import visualization\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "from deep_sort.deep_sort.tracker import Tracker\n",
    "from img2vec_pytorch import Img2Vec\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "img2vec = Img2Vec(cuda=True)\n",
    "\n",
    "print(len(train_seq_count))\n",
    "file = open('submission_deep_conf_0.5.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "counts = []\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", 0.2, 100)\n",
    "for index, seq_row in tqdm(train_seq_count.iterrows()) :\n",
    "    tracker = Tracker(metric)\n",
    "    results = []\n",
    "    track_list = []\n",
    "    img_rows = df_train.loc[df_train.seq_id == seq_row[\"seq_id\"]]\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        detections = df_detection.loc[df_detection.file == ('train/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        #image = Image.open(TRAIN_DIR+img_row['file_name'])\n",
    "        #print(detections[0][\"bbox\"])\n",
    "        #x = detections[0][\"bbox\"][1]+detections[0][\"bbox\"][0]\n",
    "        #detections[0][\"bbox\"][0]:x\n",
    "        #print(x)\n",
    "        #print(image[0:int(image.shape[1]*x),3:5,:])\n",
    "        #,detections[0][\"bbox\"][2]-detections[0][\"bbox\"][3]:detections[0][\"bbox\"][2],:\n",
    "        detections = [Detection(tuple(det[\"bbox\"]), det[\"conf\"], []) for det in detections if det[\"conf\"] > 0.5]\n",
    "#         boxes = np.array([d for d in filtered_det])\n",
    "#         scores = np.array([d.confidence for d in detections])\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            results.append(track.track_id)\n",
    "    count = len(set(results))   \n",
    "    counts.append(count)\n",
    "    file.write('\\n' + str(seq_row[\"seq_id\"]) + ',' + str(count))\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b540c44-92a7-49f8-9cd8-5be519c2bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = df_train.loc[df_train.seq_id == '302d5988-7d42-11eb-8fb5-0242ac1c0002']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6315b633-9dce-4c88-b3ea-6874691365de",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.read_csv('submission_deep_rest_test0.5.txt')[\"Predicted\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6cd8a4c-4265-4d9e-bc9d-6311ed9272bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.read_csv('submission_deep_baseline.txt')[\"Predicted\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90b39f8e-0dc3-44c8-817c-6cf1d83e6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.mean(np.abs((np.array(counts) - train_seq_count[\"count\"].to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580b205-9c30-495f-9dc7-3f63ab4192e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5362d19a-9de0-4c5c-9abe-73c2880e925d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7185393258426966\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51d15624-e33c-49f6-8694-b77b150a6d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.998, 0.998, 0.988, 0.98, 0.951, 0.693, 0.38, 0.202, 0.202, 0.2, 0.174]\n",
      "[0.998, 0.992, 0.989, 0.989, 0.984, 0.965, 0.955, 0.375, 0.333, 0.271, 0.171, 0.162]\n",
      "[0.998, 0.993, 0.993, 0.977, 0.952, 0.947, 0.85, 0.668, 0.476, 0.353, 0.337, 0.247, 0.176, 0.169, 0.123, 0.121, 0.104]\n",
      "[0.999, 0.998, 0.995, 0.993, 0.977, 0.965, 0.774, 0.138, 0.132, 0.126, 0.104]\n",
      "[0.998, 0.996, 0.989, 0.988, 0.984, 0.969, -0.879, 0.733, 0.615, 0.353, 0.315, 0.277, 0.159, 0.159, 0.13, 0.124, 0.111]\n",
      "[0.998, 0.998, 0.996, 0.987, 0.986, 0.978, -0.92, 0.506, 0.362, 0.198, 0.197, 0.184, 0.164, 0.103]\n",
      "[0.999, 0.997, 0.994, 0.992, 0.959, -0.899, 0.64, 0.621, 0.464, 0.374, 0.364, 0.118, 0.109]\n"
     ]
    }
   ],
   "source": [
    "for index, img_row in img_rows.iterrows():\n",
    "    detections = df_detection.loc[df_detection.file == ('train/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "    print([det[\"conf\"] for det in detections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75f3a70-9b88-46de-8802-adda1bba522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>location</th>\n",
       "      <th>seq_num_frames</th>\n",
       "      <th>seq_frame_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77815</th>\n",
       "      <td>94a57b08-21bc-11ea-a13a-137349068a90</td>\n",
       "      <td>302d5988-7d42-11eb-8fb5-0242ac1c0002</td>\n",
       "      <td>94a57b08-21bc-11ea-a13a-137349068a90.jpg</td>\n",
       "      <td>170</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77816</th>\n",
       "      <td>97a3447a-21bc-11ea-a13a-137349068a90</td>\n",
       "      <td>302d5988-7d42-11eb-8fb5-0242ac1c0002</td>\n",
       "      <td>97a3447a-21bc-11ea-a13a-137349068a90.jpg</td>\n",
       "      <td>170</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77817</th>\n",
       "      <td>924d3828-21bc-11ea-a13a-137349068a90</td>\n",
       "      <td>302d5988-7d42-11eb-8fb5-0242ac1c0002</td>\n",
       "      <td>924d3828-21bc-11ea-a13a-137349068a90.jpg</td>\n",
       "      <td>170</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77818</th>\n",
       "      <td>8be47ab4-21bc-11ea-a13a-137349068a90</td>\n",
       "      <td>302d5988-7d42-11eb-8fb5-0242ac1c0002</td>\n",
       "      <td>8be47ab4-21bc-11ea-a13a-137349068a90.jpg</td>\n",
       "      <td>170</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77819</th>\n",
       "      <td>8e8e4e02-21bc-11ea-a13a-137349068a90</td>\n",
       "      <td>302d5988-7d42-11eb-8fb5-0242ac1c0002</td>\n",
       "      <td>8e8e4e02-21bc-11ea-a13a-137349068a90.jpg</td>\n",
       "      <td>170</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77820</th>\n",
       "      <td>8e63e9fa-21bc-11ea-a13a-137349068a90</td>\n",
       "      <td>302d5988-7d42-11eb-8fb5-0242ac1c0002</td>\n",
       "      <td>8e63e9fa-21bc-11ea-a13a-137349068a90.jpg</td>\n",
       "      <td>170</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77821</th>\n",
       "      <td>8a5f4480-21bc-11ea-a13a-137349068a90</td>\n",
       "      <td>302d5988-7d42-11eb-8fb5-0242ac1c0002</td>\n",
       "      <td>8a5f4480-21bc-11ea-a13a-137349068a90.jpg</td>\n",
       "      <td>170</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "77815  94a57b08-21bc-11ea-a13a-137349068a90   \n",
       "77816  97a3447a-21bc-11ea-a13a-137349068a90   \n",
       "77817  924d3828-21bc-11ea-a13a-137349068a90   \n",
       "77818  8be47ab4-21bc-11ea-a13a-137349068a90   \n",
       "77819  8e8e4e02-21bc-11ea-a13a-137349068a90   \n",
       "77820  8e63e9fa-21bc-11ea-a13a-137349068a90   \n",
       "77821  8a5f4480-21bc-11ea-a13a-137349068a90   \n",
       "\n",
       "                                     seq_id  \\\n",
       "77815  302d5988-7d42-11eb-8fb5-0242ac1c0002   \n",
       "77816  302d5988-7d42-11eb-8fb5-0242ac1c0002   \n",
       "77817  302d5988-7d42-11eb-8fb5-0242ac1c0002   \n",
       "77818  302d5988-7d42-11eb-8fb5-0242ac1c0002   \n",
       "77819  302d5988-7d42-11eb-8fb5-0242ac1c0002   \n",
       "77820  302d5988-7d42-11eb-8fb5-0242ac1c0002   \n",
       "77821  302d5988-7d42-11eb-8fb5-0242ac1c0002   \n",
       "\n",
       "                                      file_name  location  seq_num_frames  \\\n",
       "77815  94a57b08-21bc-11ea-a13a-137349068a90.jpg       170               7   \n",
       "77816  97a3447a-21bc-11ea-a13a-137349068a90.jpg       170               7   \n",
       "77817  924d3828-21bc-11ea-a13a-137349068a90.jpg       170               7   \n",
       "77818  8be47ab4-21bc-11ea-a13a-137349068a90.jpg       170               7   \n",
       "77819  8e8e4e02-21bc-11ea-a13a-137349068a90.jpg       170               7   \n",
       "77820  8e63e9fa-21bc-11ea-a13a-137349068a90.jpg       170               7   \n",
       "77821  8a5f4480-21bc-11ea-a13a-137349068a90.jpg       170               7   \n",
       "\n",
       "       seq_frame_num  \n",
       "77815              0  \n",
       "77816              1  \n",
       "77817              2  \n",
       "77818              3  \n",
       "77819              4  \n",
       "77820              5  \n",
       "77821              6  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187f4571-8b39-41be-acbd-86567f54e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd7a6c-1684-4d6b-9d1b-e352471b6a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83235a-4e0f-4d62-8aab-00e4ee49bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f8878e3-1026-4799-9392-78a8a582fcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 7789cd0] created deep_sort_predictions notebook\n",
      " 3 files changed, 318 insertions(+), 9 deletions(-)\n",
      " create mode 100644 .ipynb_checkpoints/deep_sort_predictions-checkpoint.ipynb\n",
      " create mode 100644 deep_sort_predictions.ipynb\n"
     ]
    }
   ],
   "source": [
    "!git commit -am \"created deep_sort_predictions notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c182b-761a-4fc2-95a7-89b663133dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-iwildcam]",
   "language": "python",
   "name": "conda-env-.conda-iwildcam-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
