{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1939e124-0e14-4fc3-a379-f4fac4d62fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ce5c2-0391-4e64-9b4a-171e31cff5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -am \"added 1st rank submissions by merging baseline w oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec0c0aa0-3c31-4897-95c0-5941098a408f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261428/261428 [00:00<00:00, 1260411.11it/s]\n"
     ]
    }
   ],
   "source": [
    "META_DIR = \"data/metadata/metadata/\"\n",
    "TRAIN_DIR = \"data/train/train/\"\n",
    "TEST_DIR = \"data/test/test/\"\n",
    "\n",
    "test_data = json.load(open(META_DIR + 'iwildcam2022_test_information.json'))\n",
    "train_data = json.load(open(META_DIR + 'iwildcam2022_train_annotations.json'))\n",
    "\n",
    "#Test images\n",
    "df_test = pd.DataFrame({'id': [item['id'] for item in test_data['images']],\n",
    "                                'seq_id': [item['seq_id'] for item in test_data['images']],\n",
    "                                'file_name': [item['file_name'] for item in test_data['images']],\n",
    "                                'location': [item['location'] for item in test_data['images']],\n",
    "                                'seq_num_frames': [item['seq_num_frames'] for item in test_data['images']],\n",
    "                                'seq_frame_num': [item['seq_frame_num'] for item in test_data['images']]})\n",
    "#Train images\n",
    "df_train = pd.DataFrame({'id': [item['id'] for item in train_data['images']],\n",
    "                                'seq_id': [item['seq_id'] for item in train_data['images']],\n",
    "                                'file_name': [item['file_name'] for item in train_data['images']],\n",
    "                                'location': [item['location'] for item in train_data['images']],\n",
    "                                'seq_num_frames': [item['seq_num_frames'] for item in train_data['images']],\n",
    "                                'seq_frame_num': [item['seq_frame_num'] for item in train_data['images']]})\n",
    "# Detection for train test\n",
    "detections = json.load(open(META_DIR+\"iwildcam2022_mdv4_detections.json\"))['images']\n",
    "det_dict = dict()\n",
    "for detection in tqdm(detections):\n",
    "    det_dict[detection['file']] = detection['detections']\n",
    "df_detection = pd.DataFrame({'file': [item['file'] for item in detections],\n",
    "                                'detections': [item['detections'] for item in detections]})\n",
    "# Test sequence ids\n",
    "test_sequence_ids = pd.unique(df_test['seq_id'])\n",
    "# Train sequence id and count\n",
    "train_seq_count = pd.read_csv(META_DIR+\"train_sequence_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa6bb7-0ead-4d9e-83dd-0c7b9f682704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1780it [05:27,  5.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from deep_sort.deep_sort.ocsort import OCSort\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "file = open('submission_oc_det0.4.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "counts = []\n",
    "for index, seq_row in tqdm(train_seq_count.iterrows()) :\n",
    "    #print(\"######################### NEW SEQ ##################\")\n",
    "    #print(seq_row[\"seq_id\"])\n",
    "    img_rows = df_train.loc[df_train.seq_id == seq_row[\"seq_id\"]]\n",
    "    tracker = OCSort(det_thresh=0.4, max_age=30, min_hits=3, \n",
    "        iou_threshold=0, delta_t=3, asso_func=\"iou\", inertia=0.2, use_byte=False)\n",
    "    #results = []\n",
    "    outputs = []\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        image = Image.open(TRAIN_DIR+img_row['file_name'])\n",
    "        detections = df_detection.loc[df_detection.file == ('train/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        dets = np.zeros((1, 4))\n",
    "        if detections:\n",
    "            dets = np.array([d[\"bbox\"] for d in detections])\n",
    "        cates = np.array([d[\"category\"] for d in detections])\n",
    "        confs = np.array([d[\"conf\"] for d in detections])\n",
    "        output = tracker.update_public(dets, cates, confs)\n",
    "        #print(\"detections\", detections)\n",
    "        #print(\"return\", output)\n",
    "        if not output.size == 0:\n",
    "            outputs.append(output[:,4].astype('float64').max())\n",
    "        \n",
    "    count = len(tracker.trackers)\n",
    "    if not len(outputs) == 0:\n",
    "        counts.append(max(outputs))\n",
    "    else:\n",
    "        counts.append(0)\n",
    "    #print(count, max(outputs))\n",
    "    file.write('\\n' + str(seq_row[\"seq_id\"]) + ',' + str(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfefd1ef-80fb-4fdf-8a83-f54ab3799e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "347it [01:11,  5.62it/s]/home/briancy2/iwildcam2022/deep_sort/deep_sort/ocsort.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "  h = x[2] / w\n",
      "1780it [05:32,  5.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from deep_sort.deep_sort.ocsort import OCSort\n",
    "from deep_sort.application_util.preprocessing import non_max_suppression\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "file = open('submission_oc_det0.5_sup.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "counts = []\n",
    "for index, seq_row in tqdm(train_seq_count.iterrows()) :\n",
    "    #print(\"######################### NEW SEQ ##################\")\n",
    "    img_rows = df_train.loc[df_train.seq_id == seq_row[\"seq_id\"]]\n",
    "    tracker = OCSort(det_thresh=0.5, max_age=30, min_hits=3, \n",
    "        iou_threshold=0, delta_t=3, asso_func=\"iou\", inertia=0.2, use_byte=False)\n",
    "    #results = []\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        image = Image.open(TRAIN_DIR+img_row['file_name'])\n",
    "        detections = df_detection.loc[df_detection.file == ('train/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        boxes = np.array([d[\"bbox\"] for d in detections])\n",
    "        scores = np.array([d[\"conf\"] for d in detections])\n",
    "        indices = non_max_suppression(boxes, 0.1, scores)\n",
    "        detections = [detections[i] for i in indices]\n",
    "        dets = np.zeros((1, 4))\n",
    "        if detections:\n",
    "            dets = np.array([d[\"bbox\"] for d in detections])\n",
    "        cates = np.array([d[\"category\"] for d in detections])\n",
    "        confs = np.array([d[\"conf\"] for d in detections])\n",
    "        output = tracker.update_public(dets, cates, confs)\n",
    "        if not output.size == 0:\n",
    "            outputs.append(output[:,4].astype('float64').max())\n",
    "    #count = len(tracker.trackers)\n",
    "    if not len(outputs) == 0:\n",
    "        counts.append(max(outputs))\n",
    "    else:\n",
    "        counts.append(0)\n",
    "    file.write('\\n' + str(seq_row[\"seq_id\"]) + ',' + str(count))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37f83307-3f59-4147-a27c-b96a4544911f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1780it [05:22,  5.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from deep_sort.deep_sort.ocsort import OCSort\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "file = open('submission_oc_det0.5-2.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "counts = []\n",
    "i = 0\n",
    "for index, seq_row in tqdm(train_seq_count.iterrows()) :\n",
    "    #print(\"######################### NEW SEQ ##################\")\n",
    "    #print(seq_row[\"seq_id\"])\n",
    "    img_rows = df_train.loc[df_train.seq_id == seq_row[\"seq_id\"]]\n",
    "    tracker = OCSort(det_thresh=0.8, max_age=30, min_hits=3, \n",
    "        iou_threshold=0, delta_t=3, asso_func=\"iou\", inertia=0.2, use_byte=False)\n",
    "    trackers = []\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        image = Image.open(TRAIN_DIR+img_row['file_name'])\n",
    "        detections = df_detection.loc[df_detection.file == ('train/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        dets = np.zeros((1, 5))\n",
    "        detections = [d[\"bbox\"] + [d[\"conf\"]] for d in detections if d[\"conf\"] < 0.95]\n",
    "        if detections:\n",
    "            dets = np.array(detections)\n",
    "        output = tracker.update(dets, image.size, image.size)\n",
    "        #print([t.id for t in tracker.trackers])\n",
    "        trackers.extend(tracker.trackers)\n",
    "    count = len(set([t.id for t in trackers]))\n",
    "    #print(count)\n",
    "    #print(results[i])                         \n",
    "    #print(train_seq_count[\"count\"].iloc[i])\n",
    "    i += 1\n",
    "    file.write('\\n' + str(seq_row[\"seq_id\"]) + ',' + str(count+results[i-1]))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48c9ced6-5834-4da6-bf91-2986cee80f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11028/11028 [21:08<00:00,  8.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from deep_sort.deep_sort.ocsort import OCSort\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "file = open('submission_res93.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "for sequence_id in tqdm(test_sequence_ids):\n",
    "    d_count = []\n",
    "    img_rows = df_test.loc[df_test.seq_id == sequence_id]\n",
    "\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        #image = Image.open(TEST_DIR+img_row['file_name'])\n",
    "        detections = df_detection.loc[df_detection.file == ('test/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        d_count.append(len([d for d in detections if d[\"conf\"] > 0.93]))\n",
    "    count = max(d_count)\n",
    "    file.write('\\n' + str(sequence_id) + ',' + str(count))\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d7f39f8-62ea-4fe6-b174-5f047275b387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-dcc49843.pth\" to /home/briancy2/.cache/torch/hub/checkpoints/efficientnet_b7_lukemelas-dcc49843.pth\n",
      "100%|██████████| 255M/255M [00:02<00:00, 96.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1780it [24:42,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from deep_sort.application_util import preprocessing\n",
    "from deep_sort.application_util import visualization\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "from deep_sort.deep_sort.tracker import Tracker\n",
    "from img2vec_pytorch import Img2Vec\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "img2vec = Img2Vec(cuda=True, model='efficientnet_b7')\n",
    "\n",
    "print(len(train_seq_count))\n",
    "file = open('submission_deep_conf_0.5_eff.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "counts = []\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", 0, 100)\n",
    "i = 0\n",
    "for index, seq_row in tqdm(train_seq_count.iterrows()) :\n",
    "    tracker = Tracker(metric)\n",
    "    r = []\n",
    "    track_list = []\n",
    "    img_rows = df_train.loc[df_train.seq_id == seq_row[\"seq_id\"]]\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        detections = df_detection.loc[df_detection.file == ('train/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        image = Image.open(TRAIN_DIR+img_row['file_name'])\n",
    "        #print(detections[0][\"bbox\"])\n",
    "        #x = detections[0][\"bbox\"][1]+detections[0][\"bbox\"][0]\n",
    "        #detections[0][\"bbox\"][0]:x\n",
    "        #print(x)\n",
    "        #print(image[0:int(image.shape[1]*x),3:5,:])\n",
    "        #,detections[0][\"bbox\"][2]-detections[0][\"bbox\"][3]:detections[0][\"bbox\"][2],:\n",
    "        detections = [Detection(tuple(det[\"bbox\"]), det[\"conf\"], []) for det in detections if det[\"conf\"] < 0.95]\n",
    "#         boxes = np.array([d for d in filtered_det])\n",
    "#         scores = np.array([d.confidence for d in detections])\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            r.append(track.track_id)\n",
    "    count = len(set(r)) + results[i]\n",
    "    counts.append(count)\n",
    "    file.write('\\n' + str(seq_row[\"seq_id\"]) + ',' + str(count))\n",
    "    i += 1\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c820199-c14a-4871-9c27-168f74c47f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.mean(np.abs((np.array(counts) - train_seq_count[\"count\"].to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d8bce7c-06c2-4033-92cd-3066d3377aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2915730337078652\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1037782-f42a-4946-bc41-a28b0a0bf7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -7   4]\n",
      " [ -6  15]\n",
      " [ -5  24]\n",
      " [ -4  36]\n",
      " [ -3  49]\n",
      " [ -2  95]\n",
      " [ -1 211]\n",
      " [  0 680]\n",
      " [  1 322]\n",
      " [  2 166]\n",
      " [  3  89]\n",
      " [  4  43]\n",
      " [  5  26]\n",
      " [  6   9]\n",
      " [  7   2]\n",
      " [  8   7]\n",
      " [  9   1]\n",
      " [ 13   1]]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(np.unique((np.array(counts) - train_seq_count[\"count\"]), return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6b0d1bd-92f3-432e-9e3e-46d8583ed608",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.mean(np.abs(np.array(np.minimum(counts, np.array(results))) - train_seq_count[\"count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "335a86ae-9d11-4ee4-b48f-867cccbc7c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq_count[\"count\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8337f70f-8607-4b4b-9409-7f8b17d0826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0016853932584269\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.abs(results - train_seq_count[\"count\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53abfd10-9a4f-4e45-9752-861ea4981c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0016853932584269\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be8d5db-e2d5-4095-b35e-44b59877b81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -7  12]\n",
      " [ -6  48]\n",
      " [ -5  44]\n",
      " [ -4  49]\n",
      " [ -3  62]\n",
      " [ -2 166]\n",
      " [ -1 384]\n",
      " [  0 930]\n",
      " [  1  77]\n",
      " [  2   8]]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(np.unique((np.array(results) - train_seq_count[\"count\"]), return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f60b0a-b0fb-4547-816e-6736f91352d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1477fd48-6a59-430f-a03b-5fb57606a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780\n"
     ]
    }
   ],
   "source": [
    "print(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb81f7b1-b2d1-4f6e-be2d-2da30bf7b31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9629213483146067\n",
      "[[ -7   2]\n",
      " [ -6   7]\n",
      " [ -5  22]\n",
      " [ -4  48]\n",
      " [ -3  38]\n",
      " [ -2  78]\n",
      " [ -1 235]\n",
      " [  0 703]\n",
      " [  1 482]\n",
      " [  2 133]\n",
      " [  3  25]\n",
      " [  4   7]]\n"
     ]
    }
   ],
   "source": [
    "counts_oc = pd.read_csv('submission_oc_det0.5-2.txt')[\"Predicted\"].to_numpy()\n",
    "print(np.mean(np.abs((np.array(counts_oc) - train_seq_count[\"count\"].to_numpy()))))\n",
    "print(np.asarray(np.unique((np.array(counts_oc) - train_seq_count[\"count\"]), return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd13560a-e69e-4900-9317-7612193a635c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1 408]\n",
      " [  2 307]\n",
      " [  3 351]\n",
      " [  4 259]\n",
      " [  5 156]\n",
      " [  6  71]\n",
      " [  7  31]\n",
      " [  8  15]\n",
      " [  9 176]\n",
      " [ 10   6]]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(np.unique(train_seq_count[\"count\"], return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "299a3059-585d-4d40-9cc3-4c052e68d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_vals = {}\n",
    "n = np.abs((np.array(counts_oc) - train_seq_count[\"count\"]))\n",
    "for i, count in enumerate(train_seq_count[\"count\"]):\n",
    "    if count in true_vals.keys():\n",
    "        true_vals[count] += n[i]\n",
    "    else:\n",
    "        true_vals[count] = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52f423c6-58d3-454f-a538-749c19370146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9: 520, 10: 6, 3: 185, 2: 192, 1: 299, 5: 137, 4: 208, 7: 45, 6: 86, 8: 20}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a9925-9127-4433-a723-209127c0db32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bd40e31-0d7e-4705-8481-9a9cda7cbdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0910112359550561\n",
      "[[ -8   1]\n",
      " [ -7  18]\n",
      " [ -6  51]\n",
      " [ -5  45]\n",
      " [ -4  47]\n",
      " [ -3  69]\n",
      " [ -2 190]\n",
      " [ -1 416]\n",
      " [  0 864]\n",
      " [  1  72]\n",
      " [  2   7]]\n"
     ]
    }
   ],
   "source": [
    "merged_results = np.minimum(counts_oc, np.array(results))\n",
    "print(np.mean(np.abs(merged_results - train_seq_count[\"count\"].to_numpy())))\n",
    "print(np.asarray(np.unique((np.array(merged_results) - train_seq_count[\"count\"]), return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a264d-dfd4-4dd9-91b7-4c44b05d590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results = np.minimum(counts_oc, np.array(results))\n",
    "print(np.mean(np.abs(merged_results - train_seq_count[\"count\"].to_numpy())))\n",
    "print(np.asarray(np.unique((np.array(merged_results) - train_seq_count[\"count\"]), return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6994ff66-faae-403f-be5a-a8eadd3aeb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1780,) (1780,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(results).shape, counts_oc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04cd34d4-6dc5-4af6-af32-4de0e7b835d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3780898876404495\n",
      "[[ -9   3]\n",
      " [ -8  20]\n",
      " [ -7  18]\n",
      " [ -6  36]\n",
      " [ -5  41]\n",
      " [ -4  58]\n",
      " [ -3  96]\n",
      " [ -2 167]\n",
      " [ -1 432]\n",
      " [  0 604]\n",
      " [  1 216]\n",
      " [  2  63]\n",
      " [  3  18]\n",
      " [  4   5]\n",
      " [  5   1]\n",
      " [  6   2]]\n"
     ]
    }
   ],
   "source": [
    "counts_deep = pd.read_csv('submission_deep_conf_0.5_nn0.txt')[\"Predicted\"].to_numpy()\n",
    "print(np.mean(np.abs((np.array(counts_deep) - train_seq_count[\"count\"].to_numpy()))))\n",
    "print(np.asarray(np.unique((np.array(counts_deep) - train_seq_count[\"count\"]), return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fd651f4-1311-44b5-8c19-d17443231c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1258426966292134\n",
      "[[ -8   3]\n",
      " [ -7  21]\n",
      " [ -6  48]\n",
      " [ -5  47]\n",
      " [ -4  55]\n",
      " [ -3  69]\n",
      " [ -2 184]\n",
      " [ -1 432]\n",
      " [  0 845]\n",
      " [  1  69]\n",
      " [  2   7]]\n"
     ]
    }
   ],
   "source": [
    "merged_deep = np.minimum(counts_deep, np.array(results))\n",
    "print(np.mean(np.abs(merged_results - train_seq_count[\"count\"].to_numpy())))\n",
    "print(np.asarray(np.unique((np.array(merged_results) - train_seq_count[\"count\"]), return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cfdd05a-eab3-4b17-ae3e-eb506988e69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4797752808988764\n",
      "[[ -8   1]\n",
      " [ -7   8]\n",
      " [ -6  22]\n",
      " [ -5  29]\n",
      " [ -4  39]\n",
      " [ -3  65]\n",
      " [ -2  99]\n",
      " [ -1 257]\n",
      " [  0 645]\n",
      " [  1 331]\n",
      " [  2 117]\n",
      " [  3  62]\n",
      " [  4  24]\n",
      " [  5  22]\n",
      " [  6  20]\n",
      " [  7   8]\n",
      " [  8   9]\n",
      " [  9   7]\n",
      " [ 10   1]\n",
      " [ 11   4]\n",
      " [ 12   3]\n",
      " [ 14   2]\n",
      " [ 18   4]\n",
      " [ 29   1]]\n"
     ]
    }
   ],
   "source": [
    "counts_deep0 = pd.read_csv('submission_oc_det0.5.txt')[\"Predicted\"].to_numpy()\n",
    "print(np.mean(np.abs((np.array(counts_deep0) - train_seq_count[\"count\"].to_numpy()))))\n",
    "print(np.asarray(np.unique((np.array(counts_deep0) - train_seq_count[\"count\"]), return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ebaa1-b6ab-40a0-bbe0-b36c5a9248ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d42ac-d76f-4e98-a9b5-8cef71a4c6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd6d956c-fb1e-42f7-82b4-44578543d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.mean(np.abs((np.array(results) - train_seq_count[\"count\"].to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7de7770-3f61-4aec-b5b9-c0579aa97e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6578651685393258\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17cc0619-31e9-4b45-bea9-c744d1eef181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1780it [04:34,  6.48it/s]\n"
     ]
    }
   ],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "file = open('submission_mdv40.95.txt', 'w')\n",
    "file.write(\"Id,Predicted\")\n",
    "results = []\n",
    "for index, seq_row in tqdm(train_seq_count.iterrows()) :\n",
    "    base_counts = []\n",
    "    img_rows = df_train.loc[df_train.seq_id == seq_row[\"seq_id\"]]\n",
    "    for index, img_row in img_rows.iterrows():\n",
    "        #image = Image.open(TRAIN_DIR+img_row['file_name'])\n",
    "        detections = df_detection.loc[df_detection.file == ('train/'+img_row['file_name'])]['detections'].to_list()[0]\n",
    "        l = [d for d in detections if d[\"conf\"] > 0.95]\n",
    "        count=len(l)\n",
    "        base_counts.append(count)\n",
    "    results.append(max(base_counts))\n",
    "    file.write('\\n' + str(seq_row[\"seq_id\"]) + ',' + str(base_counts))\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb436df-4842-4e5b-af00-859734940f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-iwildcam]",
   "language": "python",
   "name": "conda-env-.conda-iwildcam-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
